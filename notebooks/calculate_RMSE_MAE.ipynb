{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31651182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Define a function to interpolate missing time steps in a DataFrame \n",
    "def interpolate_missing_time_steps(df):\n",
    "    time_column = df.columns[0]  # Extract the time column name\n",
    "    probability_columns = df.columns[1:]  # Extract the names of the probability columns\n",
    "    df[time_column] = df[time_column].astype(int)  # Convert time values to integers\n",
    "    full_time_range = range(df[time_column].min(), df[time_column].max() + 1)  # Generate the full range of time steps\n",
    "    df_reindexed = df.set_index(time_column).reindex(full_time_range).reset_index()  # Reindex the DataFrame to fill missing time steps\n",
    "    df_reindexed[probability_columns] = df_reindexed[probability_columns].interpolate()  # Interpolate missing probability values\n",
    "    return df_reindexed  # Return the reindexed DataFrame\n",
    "\n",
    "def lcr(df, k=1):\n",
    "    probabilities = df.iloc[:, 1:].values  # Exclude the 'Time' column for calculations\n",
    "    strengths = np.exp(k * probabilities)\n",
    "\n",
    "    # Sum strengths per row\n",
    "    sum_strengths = strengths.sum(axis=1)\n",
    "\n",
    "    # Compute Luce choice probabilities\n",
    "    luce_probs = strengths / sum_strengths[:, np.newaxis]\n",
    "    \n",
    "    # Step 2: Compute scaling factor Δ_t for each row\n",
    "    # max(act(t)): maximum activation in the current row\n",
    "    row_max = df.iloc[:, 1:].max(axis=1)\n",
    "    # max(act(overall)): global maximum activation across all rows and all items\n",
    "    global_max = df.iloc[:, 1:].max().max()\n",
    "    delta = row_max / global_max\n",
    "\n",
    "    # Step 3: Compute final fixation probabilities p(R_i) = Δ_t * L_i\n",
    "    # Convert delta to numpy array before reshaping to avoid pandas multi-dimensional indexing error\n",
    "    fixation_probs = luce_probs * delta.values[:, np.newaxis]\n",
    "\n",
    "    transformed_df = df.copy()  # Create a copy of the original DataFrame to store results\n",
    "    transformed_df.iloc[:, 1:] = fixation_probs  # Replace the original probabilities with the fixation probabilities\n",
    "    return transformed_df\n",
    "\n",
    "# Define a function to read a CSV file, preprocess data, and apply optional transformations\n",
    "def read_csv(file_path, shift=False, apply_softmax=False, tmult=1, C=5, k=1, \n",
    "             generate_cross=True, apply_lcr_first=False, apply_lcr=False, \n",
    "             normalize=False, rescale=False, rescale_first=False, do_interpolate_missing_time_steps=True,\n",
    "             bgate=False, bthresh=0.01, tmax=1000, sum_weight=1):\n",
    "    df = pd.read_csv(file_path)  # Read the CSV file into a DataFrame\n",
    "    probability_columns = df.columns[1:]  # Extract the names of the probability columns\n",
    "\n",
    "    if bgate:  # Apply bgate logic if bgate is True\n",
    "        # print(f'Applying bgate with threshold {threshold}')\n",
    "        row_sums = df[probability_columns].sum(axis=1)  # Sum probabilities across columns for each row\n",
    "        mask = row_sums < bthresh  # Identify rows where the sum of probabilities is below the threshold\n",
    "        df.loc[mask, probability_columns] = 0  # Set probabilities to 0 for those rows\n",
    "    \n",
    "    if rescale_first:  # Apply rescaling if rescale is True\n",
    "        print('Rescaling data first')\n",
    "        df_min = df[probability_columns].min().min()  # Find the minimum value across all columns\n",
    "        df_max = df[probability_columns].max().max()  # Find the maximum value across all columns\n",
    "        df[probability_columns] = (df[probability_columns] - df_min) / (df_max - df_min)  # Rescale to [0, 1]\n",
    "\n",
    "    if apply_lcr_first:  # Apply Luce choice rule if apply_lcr is True\n",
    "        print(f'Applying lcr FIRST, k = {k}')\n",
    "        print('BEFORE')\n",
    "        print(df.tail(5))\n",
    "        df = lcr(df, k=k)  # Apply LCR to the DataFrame, including only columns in the input\n",
    "        print(f'AFTER')\n",
    "        print(df.tail(5))\n",
    "        print(df.head(5))\n",
    "        \n",
    "    if 'Cross' not in df.columns:  # If the 'Cross' column is missing\n",
    "        sum_columns = ['Target', 'Cohort', 'Rhyme', 'Unrelated']  # Define columns for summation\n",
    "        sum_columns = [col for col in sum_columns if col in df.columns]  # Filter out columns that are not in the set\n",
    "        \n",
    "        if generate_cross:  # If generate_cross is true\n",
    "            total_sum = df[sum_columns].sum(axis=1)  # Calculate the total sum of probabilities for each row\n",
    "            trg_max = df['Target'].max()  # Find the maximum value in the 'Target' column\n",
    "            sum_max = total_sum.max() * sum_weight # Find the maximum value in the 'Target' column\n",
    "            max_value = sum_max\n",
    "            if rescale:\n",
    "                max_value = 1\n",
    "\n",
    "            df['Cross'] = max_value - (total_sum / sum_max)  # Scale it to decrease gradually\n",
    "            df['Cross'] = df['Cross'].clip(lower=0)  # Clip the 'Cross' values to avoid negative values\n",
    "            # # After generating or computing the 'Cross' column, add this code to ensure 'Cross' stays 0 after hitting zero\n",
    "            cross_zero_mask = df['Cross'].eq(0)  # Identify where 'Cross' reaches zero\n",
    "            df['Cross'] = df['Cross'].mask(cross_zero_mask.cumsum().gt(0), 0)  # Set 'Cross' to 0 for the rest of the timesteps\n",
    "        else:  # If generate_cross is false\n",
    "            df['Cross'] = np.nan  # Set the 'Cross' column to NaN values\n",
    "\n",
    "    if rescale:  # Apply rescaling if rescale is True\n",
    "        print('Rescaling data')\n",
    "        df_min = df[probability_columns].min().min()  # Find the minimum value across all columns\n",
    "        df_max = df[probability_columns].max().max()  # Find the maximum value across all columns\n",
    "        df[probability_columns] = (df[probability_columns] - df_min) / (df_max - df_min)  # Rescale to [0, 1]\n",
    "\n",
    "    df['Time'] = df['Time'] * tmult  # Multiply the time column by tmult factor\n",
    "    if do_interpolate_missing_time_steps:  # If interpolation is enabled\n",
    "        print('Interpolating missing time steps')\n",
    "        df = interpolate_missing_time_steps(df)  # Interpolate missing time steps\n",
    "        # input_df = df.copy(deep=True)  # Create a deep copy of the DataFrame\n",
    "\n",
    "    if apply_lcr:  # Apply Luce choice rule if apply_lcr is True\n",
    "        print(f'Applying lcr, k={k}')\n",
    "        df = lcr(df, k=k)  # Apply LCR to the DataFrame, including columns in the input AND cross if it's been added\n",
    "\n",
    "    if normalize:  # Apply normalization if normalize is True\n",
    "        row_sums = df[probability_columns].sum(axis=1)  # Get the sum for each row\n",
    "        non_zero_mask = df[probability_columns] != 0  # Create a mask where the values are non-zero\n",
    "        df[probability_columns] = df[probability_columns].where(~non_zero_mask, df[probability_columns].div(row_sums, axis=0))  # Normalize only non-zero values\n",
    "    \n",
    "    if apply_softmax:  # If softmax is enabled\n",
    "        df = softmax(df[probability_columns].values, axis=1)  # Apply standard softmax transformation\n",
    "    \n",
    "    # Trim the DataFrame to only include rows where 'Time' is less than or equal to tmax\n",
    "    df = df[df['Time'] <= tmax]\n",
    "\n",
    "    return df  # Return transformed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381514a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(human_df, comp_df, modelname):\n",
    "    \"\"\"\n",
    "    Calculates objective metrics (RMSE, MAE, Correlation) to measure the difference \n",
    "    between human and simulated fixation proportions.\n",
    "    \"\"\"\n",
    "    item_types = [\"Target\", \"Cohort\", \"Rhyme\", \"Unrelated\", \"Cross\"]\n",
    "    \n",
    "    # Align dataframes by length\n",
    "    min_len = min(len(human_df), len(comp_df))\n",
    "    h_df = human_df.iloc[:min_len].reset_index(drop=True)\n",
    "    c_df = comp_df.iloc[:min_len].reset_index(drop=True)\n",
    "    \n",
    "    metrics = {}\n",
    "    print(f\"\\n{'='*20} {modelname} Objective Metrics {'='*20}\")\n",
    "    print(f\"{'Item':<12} | {'RMSE':<10} | {'MAE':<10} | {'Correlation':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    total_sq_error = 0\n",
    "    total_abs_error = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for item in item_types:\n",
    "        if item in h_df.columns and item in c_df.columns:\n",
    "            y_true = h_df[item]\n",
    "            y_pred = c_df[item]\n",
    "            \n",
    "            # RMSE\n",
    "            mse = np.mean((y_true - y_pred)**2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # MAE\n",
    "            mae = np.mean(np.abs(y_true - y_pred))\n",
    "            \n",
    "            # Correlation\n",
    "            if y_true.std() == 0 or y_pred.std() == 0:\n",
    "                corr = 0\n",
    "            else:\n",
    "                corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "                \n",
    "            metrics[item] = {'RMSE': rmse, 'MAE': mae, 'Correlation': corr}\n",
    "            print(f\"{item:<12} | {rmse:.4f}     | {mae:.4f}     | {corr:.4f}\")\n",
    "            \n",
    "            total_sq_error += np.sum((y_true - y_pred)**2)\n",
    "            total_abs_error += np.sum(np.abs(y_true - y_pred))\n",
    "            total_count += len(y_true)\n",
    "            \n",
    "    # Global weighted metrics\n",
    "    if total_count > 0:\n",
    "        global_rmse = np.sqrt(total_sq_error / total_count)\n",
    "        global_mae = total_abs_error / total_count\n",
    "        print(\"-\" * 55)\n",
    "        print(f\"{'Overall':<12} | {global_rmse:.4f}     | {global_mae:.4f}     | {'-':<10}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def plot_fixation_proportions(human_df, comp_df, modelname, symbol_interval=25):\n",
    "    markers = {\"Target\": \"o\", \"Cohort\": \"s\", \"Rhyme\": \"^\", \"Unrelated\": \".\", \"Cross\": \"x\"}  # Define markers for different item types\n",
    "    item_types = [\"Target\", \"Cohort\", \"Rhyme\", \"Unrelated\", \"Cross\"]  # List of item types\n",
    "\n",
    "    # Define custom font sizes\n",
    "    title_fontsize = 24\n",
    "    axis_label_fontsize = 22\n",
    "    tick_labelsize = 18\n",
    "    legend_fontsize = 18\n",
    "    annotate_fontsize = 32\n",
    "    \n",
    "    # Ensure both dataframes cover the same time range for comparison\n",
    "    # We assume 'Time' is available or the index represents time. \n",
    "    # Based on read_csv, 'Time' is a column but indices are reset.\n",
    "    # Let's align them based on the minimum common length to simply truncate the longer one,\n",
    "    # or better, align by their 'Time' column if possible.\n",
    "    \n",
    "    # Assuming standard index alignment (0, 1, 2...) if they start at same relative time:\n",
    "    min_len = min(len(human_df), len(comp_df))\n",
    "    human_df_clipped = human_df.iloc[:min_len]\n",
    "    comp_df_clipped = comp_df.iloc[:min_len]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    for item in item_types:\n",
    "        if item in human_df_clipped.columns and item in comp_df_clipped.columns:\n",
    "            difference = human_df_clipped[item] - comp_df_clipped[item]\n",
    "            # Use the time column if available for x-axis, else use index\n",
    "            if 'Time' in human_df_clipped.columns:\n",
    "                x_axis = human_df_clipped['Time']\n",
    "            else:\n",
    "                x_axis = human_df_clipped.index\n",
    "                \n",
    "            ax.plot(x_axis, difference, label=item, marker=markers.get(item, ''), markevery=symbol_interval)\n",
    "\n",
    "    ax.set_title('Differences (human - simulated)', fontsize=title_fontsize)\n",
    "    ax.set_xlabel('Time step', fontsize=axis_label_fontsize)\n",
    "    ax.set_ylabel('Fixation proportion difference', fontsize=axis_label_fontsize)\n",
    "    ax.tick_params(axis='both', labelsize=tick_labelsize)  # Set tick label size\n",
    "    ax.legend(fontsize=legend_fontsize)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    fig.suptitle(f'Differences (human - simulated) for {modelname}', fontsize=title_fontsize)\n",
    "    fig.savefig(f'DIFF/differences_{modelname}.png')\n",
    "    \n",
    "    # show human df and comp df  \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 24))\n",
    "    axes[0].set_title('Human Data', fontsize=title_fontsize)\n",
    "    axes[1].set_title('Simulated Data', fontsize=title_fontsize)\n",
    "    for item in item_types:\n",
    "        if item in human_df_clipped.columns:\n",
    "            axes[0].plot(human_df_clipped['Time'], human_df_clipped[item], label=item, marker=markers.get(item, ''), markevery=symbol_interval)\n",
    "        if item in comp_df_clipped.columns:\n",
    "            # axes[1].axvline(x=480, color='gray', linestyle='--', alpha=0.5, label=\"theoratical earliest RT\")  # Add vertical line at the start of the time range\n",
    "            axes[1].plot(comp_df_clipped['Time'], comp_df_clipped[item], label=item, marker=markers.get(item, ''), markevery=symbol_interval)\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Time step', fontsize=axis_label_fontsize)\n",
    "        ax.set_ylabel('Fixation proportion', fontsize=axis_label_fontsize)\n",
    "        ax.tick_params(axis='both', labelsize=tick_labelsize)  # Set tick label size\n",
    "        ax.legend(fontsize=legend_fontsize)\n",
    "    fig.savefig(f'OUT/fixation_proportions_{modelname}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858d5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /home/fie24002/earshot_nn to sys.path\n",
      "Interpolating missing time steps\n",
      "Processing csv: experiments/en_words_ku_baseline/Baseline/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== baseline Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.0952     | 0.0756     | 0.9757\n",
      "Cohort       | 0.0308     | 0.0240     | 0.8750\n",
      "Rhyme        | 0.0379     | 0.0313     | 0.7874\n",
      "Unrelated    | 0.0208     | 0.0169     | -0.1813\n",
      "Cross        | 0.1007     | 0.0886     | 0.9934\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.0664     | 0.0473     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_causal_cnn/causal-cnn/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== causal-cnn Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.1247     | 0.0997     | 0.9518\n",
      "Cohort       | 0.0424     | 0.0354     | 0.7343\n",
      "Rhyme        | 0.0626     | 0.0547     | 0.8011\n",
      "Cross        | 0.0903     | 0.0676     | 0.9825\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.0858     | 0.0644     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_causal_trans/causal-trans/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== causal-trans Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.1071     | 0.0911     | 0.9731\n",
      "Cohort       | 0.0420     | 0.0381     | 0.7989\n",
      "Rhyme        | 0.0793     | 0.0636     | 0.7399\n",
      "Unrelated    | 0.0225     | 0.0202     | -0.5965\n",
      "Cross        | 0.0591     | 0.0526     | 0.9939\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.0686     | 0.0531     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_causal_rcnn/causal-rcnn/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== causal-rcnn Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.1050     | 0.0848     | 0.9674\n",
      "Cohort       | 0.0355     | 0.0280     | 0.8189\n",
      "Rhyme        | 0.0379     | 0.0264     | 0.8155\n",
      "Unrelated    | 0.0209     | 0.0176     | -0.5247\n",
      "Cross        | 0.0679     | 0.0488     | 0.9891\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.0613     | 0.0411     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_causal_2lstm/causal-2LSTM/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== causal-2lstm Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.0818     | 0.0660     | 0.9796\n",
      "Cohort       | 0.0315     | 0.0250     | 0.8643\n",
      "Rhyme        | 0.0668     | 0.0562     | 0.8159\n",
      "Unrelated    | 0.0210     | 0.0178     | 0.2092\n",
      "Cross        | 0.0819     | 0.0726     | 0.9957\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.0621     | 0.0475     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_2lstmbi/noncausal-2LSTM/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== noncausal-2lstm Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.3303     | 0.2801     | 0.7876\n",
      "Cohort       | 0.0613     | 0.0439     | 0.6158\n",
      "Rhyme        | 0.0952     | 0.0912     | 0.7139\n",
      "Unrelated    | 0.0214     | 0.0177     | -0.6126\n",
      "Cross        | 0.3740     | 0.2950     | 0.8647\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.2290     | 0.1456     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_rcnn/noncausal-rcnn/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== noncausal-rcnn Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.1720     | 0.1382     | 0.9101\n",
      "Cohort       | 0.0527     | 0.0415     | 0.5214\n",
      "Rhyme        | 0.0696     | 0.0634     | 0.8348\n",
      "Unrelated    | 0.0210     | 0.0178     | -0.5779\n",
      "Cross        | 0.1412     | 0.1062     | 0.9622\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.1073     | 0.0734     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_cnn/noncausal-cnn/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== noncausal-cnn Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.2880     | 0.2275     | 0.7230\n",
      "Cohort       | 0.0636     | 0.0438     | 0.5460\n",
      "Rhyme        | 0.0370     | 0.0295     | 0.7678\n",
      "Cross        | 0.2983     | 0.2230     | 0.8710\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.2106     | 0.1310     | -         \n",
      "============================================================\n",
      "\n",
      "Processing csv: experiments/en_words_ku_trans/noncausal-trans/training/competition_mean.csv\n",
      "Interpolating missing time steps\n",
      "\n",
      "==================== noncausal-trans Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.4328     | 0.3544     | 0.8367\n",
      "Cohort       | 0.0641     | 0.0490     | 0.6906\n",
      "Rhyme        | 0.0821     | 0.0778     | 0.7929\n",
      "Unrelated    | 0.0214     | 0.0171     | 0.4377\n",
      "Cross        | 0.4467     | 0.3277     | 0.9070\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.2822     | 0.1652     | -         \n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Check if we are in the 'notebooks' directory and define project root accordingly\n",
    "if os.path.basename(current_dir) == 'notebooks':\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    # Assume we are already at the project root or handle other structures as needed\n",
    "    project_root = current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    print(f\"Adding {project_root} to sys.path\")\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "model2res = {\n",
    "    # Causal Models from train.sh\n",
    "    \"baseline\": \"experiments/en_words_ku_baseline/Baseline/training/competition_mean.csv\",\n",
    "    \"causal-cnn\": \"experiments/en_words_ku_causal_cnn/causal-cnn/training/competition_mean.csv\",\n",
    "    \"causal-trans\": \"experiments/en_words_ku_causal_trans/causal-trans/training/competition_mean.csv\",\n",
    "    \"causal-rcnn\": \"experiments/en_words_ku_causal_rcnn/causal-rcnn/training/competition_mean.csv\",\n",
    "    \"causal-2lstm\": \"experiments/en_words_ku_causal_2lstm/causal-2LSTM/training/competition_mean.csv\",\n",
    "    # \"causal-ctrans\": \"experiments/en_words_ku_causal_convtrans/causal-convtrans/training/competition_mean.csv\",\n",
    "\n",
    "    # Non-Causal Models from train.sh\n",
    "    \"noncausal-2lstm\": \"experiments/en_words_ku_2lstmbi/noncausal-2LSTM/training/competition_mean.csv\",\n",
    "    # \"noncausal-convtrans\": \"experiments/en_words_ku_convtrans/noncausal-convtrans/training/competition_mean.csv\",\n",
    "    \"noncausal-rcnn\": \"experiments/en_words_ku_rcnn/noncausal-rcnn/training/competition_mean.csv\",\n",
    "    \"noncausal-cnn\": \"experiments/en_words_ku_cnn/noncausal-cnn/training/competition_mean.csv\",\n",
    "    \"noncausal-trans\": \"experiments/en_words_ku_trans/noncausal-trans/training/competition_mean.csv\"\n",
    "}\n",
    "input_path = os.path.join(project_root, \"notebooks/INPUT/amt_lcr_from_paper.csv\")\n",
    "human_df = read_csv(input_path, normalize=False, rescale=False, tmult=1, \n",
    "                    apply_lcr_first=False, apply_lcr=False, k=1, bgate=False, bthresh=0.01, tmax=1000)\n",
    "    \n",
    "for modelname, csv_path in model2res.items():\n",
    "    print(f\"Processing csv: {csv_path}\")\n",
    "    # Construct absolute path to ensure we find the file regardless of CWD\n",
    "    full_csv_path = os.path.join(project_root, csv_path)\n",
    " \n",
    "    comp_df = read_csv(full_csv_path, normalize=False, rescale=False, tmult=10, \n",
    "                    apply_lcr_first=False, apply_lcr=False, k=1, bgate=False, bthresh=0.01, tmax=1000)\n",
    "\n",
    "    # plot_fixation_proportions(\n",
    "    #     human_df,  # \"INPUT/amt_lcr_from_paper.csv\"\n",
    "    #     comp_df, # underlying response probabilities\n",
    "    #     modelname\n",
    "    # )\n",
    "\n",
    "    # Calculate and print objective metrics\n",
    "    metrics = calculate_metrics(human_df, comp_df, modelname) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978323bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: wav2vec2\n",
      "Applying lcr FIRST, k = 6\n",
      "BEFORE\n",
      "    Time  Target  Cohort  Rhyme\n",
      "45   900  0.1483  0.0004    0.0\n",
      "46   920  0.1433  0.0005    0.0\n",
      "47   940  0.1408  0.0005    0.0\n",
      "48   960  0.1307  0.0006    0.0\n",
      "49   980  0.1374  0.0005    0.0\n",
      "AFTER\n",
      "    Time    Target    Cohort     Rhyme\n",
      "45   900  0.548708  0.225917  0.225375\n",
      "46   920  0.522948  0.222001  0.221336\n",
      "47   940  0.510286  0.219900  0.219241\n",
      "48   960  0.460313  0.210883  0.210126\n",
      "49   980  0.493262  0.216944  0.216294\n",
      "   Time  Target  Cohort  Rhyme\n",
      "0     0     0.0     0.0    0.0\n",
      "1    20     0.0     0.0    0.0\n",
      "2    40     0.0     0.0    0.0\n",
      "3    60     0.0     0.0    0.0\n",
      "4    80     0.0     0.0    0.0\n",
      "Rescaling data\n",
      "Interpolating missing time steps\n",
      "1001 981\n",
      "\n",
      "==================== wav2vec2 Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.0974     | 0.0650     | 0.9786\n",
      "Cohort       | 0.2429     | 0.1845     | -0.1017\n",
      "Rhyme        | 0.2197     | 0.1609     | 0.6599\n",
      "Cross        | 0.1811     | 0.1114     | 0.9369\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.1933     | 0.1305     | -         \n",
      "============================================================\n",
      "\n",
      "Processing model: hubert\n",
      "Applying lcr FIRST, k = 6\n",
      "BEFORE\n",
      "    Time  Target  Cohort  Rhyme\n",
      "45   900  0.1480  0.0002    0.0\n",
      "46   920  0.1537  0.0003    0.0\n",
      "47   940  0.1570  0.0004    0.0\n",
      "48   960  0.1493  0.0004    0.0\n",
      "49   980  0.1499  0.0005    0.0\n",
      "AFTER\n",
      "    Time    Target    Cohort     Rhyme\n",
      "45   900  0.516973  0.212979  0.212723\n",
      "46   920  0.545089  0.217141  0.216751\n",
      "47   940  0.561599  0.219464  0.218938\n",
      "48   960  0.523209  0.214130  0.213616\n",
      "49   980  0.526092  0.214664  0.214021\n",
      "   Time  Target  Cohort  Rhyme\n",
      "0     0     0.0     0.0    0.0\n",
      "1    20     0.0     0.0    0.0\n",
      "2    40     0.0     0.0    0.0\n",
      "3    60     0.0     0.0    0.0\n",
      "4    80     0.0     0.0    0.0\n",
      "Rescaling data\n",
      "Interpolating missing time steps\n",
      "1001 981\n",
      "\n",
      "==================== hubert Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.0868     | 0.0572     | 0.9831\n",
      "Cohort       | 0.2302     | 0.1760     | -0.0677\n",
      "Rhyme        | 0.2084     | 0.1540     | 0.6741\n",
      "Cross        | 0.1710     | 0.1065     | 0.9442\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.1825     | 0.1234     | -         \n",
      "============================================================\n",
      "\n",
      "Processing model: whisper\n",
      "Applying lcr FIRST, k = 6\n",
      "BEFORE\n",
      "    Time  Target  Cohort   Rhyme\n",
      "45   900  0.1227  0.0060  0.0448\n",
      "46   920  0.1151  0.0055  0.0456\n",
      "47   940  0.1087  0.0050  0.0456\n",
      "48   960  0.1024  0.0047  0.0435\n",
      "49   980  0.0983  0.0044  0.0391\n",
      "AFTER\n",
      "    Time    Target    Cohort     Rhyme\n",
      "45   900  0.227172  0.112787  0.142352\n",
      "46   920  0.207815  0.107668  0.136954\n",
      "47   940  0.192331  0.103236  0.131712\n",
      "48   960  0.178204  0.099160  0.125152\n",
      "49   980  0.170197  0.096888  0.119314\n",
      "   Time    Target    Cohort     Rhyme\n",
      "0     0  0.091910  0.071522  0.066914\n",
      "1    20  0.025383  0.023423  0.023521\n",
      "2    40  0.016488  0.015602  0.015866\n",
      "3    60  0.014526  0.013821  0.014106\n",
      "4    80  0.013961  0.013306  0.013613\n",
      "Rescaling data\n",
      "Interpolating missing time steps\n",
      "1001 981\n",
      "\n",
      "==================== whisper Objective Metrics ====================\n",
      "Item         | RMSE       | MAE        | Correlation\n",
      "-------------------------------------------------------\n",
      "Target       | 0.4375     | 0.3624     | 0.2783\n",
      "Cohort       | 0.1204     | 0.1051     | 0.6801\n",
      "Rhyme        | 0.1683     | 0.1521     | 0.8812\n",
      "Cross        | 0.2731     | 0.2005     | 0.8883\n",
      "-------------------------------------------------------\n",
      "Overall      | 0.2779     | 0.2050     | -         \n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2res = {\n",
    "    \"wav2vec2\": \"experiments/wav2vec2/competition_mean.csv\",\n",
    "    \"hubert\": \"experiments/hubert/facebook/hubert-large-ls960-ft/competition_mean.csv\",\n",
    "    \"whisper\": \"experiments/whisper/competition_mean.csv\"\n",
    "}\n",
    "\n",
    "for modelname, csv_path in model2res.items():\n",
    "    print(f\"Processing model: {modelname}\")\n",
    "    full_csv_path = os.path.join(project_root, csv_path)\n",
    "    comp_df = read_csv(full_csv_path, normalize=False, rescale=True, tmult=1, do_interpolate_missing_time_steps=True,\n",
    "                    apply_lcr_first=True, apply_lcr=False, k=6, bgate=False, bthresh=0.01, tmax=1000)\n",
    "\n",
    "    # plot_fixation_proportions(\n",
    "    #     human_df,  # \"INPUT/amt_lcr_from_paper.csv\"\n",
    "    #     comp_df, # underlying response probabilities\n",
    "    #     modelname\n",
    "    # )\n",
    "    print(len(human_df), len(comp_df))\n",
    "    # Calculate and print objective metrics\n",
    "    metrics = calculate_metrics(human_df, comp_df, modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c49b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
